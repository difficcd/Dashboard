import json
import urllib.request
import urllib.parse
import time
import re
from datetime import datetime, timedelta
from sentence_transformers import SentenceTransformer, util
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from dbmanage_News import SessionLocal, Bill, get_bills_by_age, insert_bill, init_db

# --- ì„¤ì • ---
API_KEY = "68da180a494a4cc3b8add2071dc95242"
client_id = "CKb4pAJ84D6tVcCvpjka"
client_secret = "5PucVvnteo"

# ì„ë² ë”© ëª¨ë¸
model = SentenceTransformer('all-MiniLM-L6-v2')
printed_titles = []
embedding_cache = {}

# ì…€ë ˆë‹ˆì›€ ì„¤ì •
options = Options()
options.add_argument("--headless")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
driver = webdriver.Chrome(options=options)

# --- í•¨ìˆ˜ ---

def get_embedding(text):
    if text not in embedding_cache:
        embedding_cache[text] = model.encode(text, convert_to_tensor=True)
    return embedding_cache[text]

def get_bill_titles_by_age(age):
    url = "https://open.assembly.go.kr/portal/openapi/TVBPMBILL11"
    bill_titles = []
    p_index = 1
    p_size = 1000

    while True:
        params = {
            "KEY": API_KEY,
            "Type": "json",
            "pIndex": p_index,
            "pSize": p_size,
            "AGE": age
        }
        full_url = f"{url}?{urllib.parse.urlencode(params)}"

        try:
            time.sleep(0.1)
            req = urllib.request.Request(full_url, headers={"User-Agent": "Mozilla/5.0"})
            response = urllib.request.urlopen(req, timeout=10)
            data = json.loads(response.read())

            items = data.get("TVBPMBILL11", [])
            rows = items[1].get("row", []) if len(items) > 1 else []

            if not rows:
                break

            for row in rows:
                bill_name = row.get("BILL_NAME", "").strip()
                if bill_name:
                    bill_titles.append(bill_name)

            if len(rows) < p_size:
                break

            p_index += 1

        except Exception as e:
            print(f"[ERROR] {age}ëŒ€ {p_index}í˜ì´ì§€ ì—ëŸ¬: {e}")
            break

    return bill_titles

def get_comment_count(news_url):
    try:
        driver.get(news_url)
        time.sleep(1)

        # ë”ë³´ê¸° ë²„íŠ¼ ê³„ì† í´ë¦­
        while True:
            try:
                more_btn = driver.find_element(By.CLASS_NAME, "u_cbox_btn_more")
                driver.execute_script("arguments[0].click();", more_btn)
                time.sleep(0.5)
            except:
                break

        comments = driver.find_elements(By.CSS_SELECTOR, "span.u_cbox_contents")
        count = len(comments)

        return count

    except Exception as e:
        print(f"   [ëŒ“ê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨] {news_url} â†’ {e}")
        return 0



def search_news_unique(title, sim_threshold=0.6):  # ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •
    cleaned_title = re.sub(r'ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ.*|ì „ë¶€ê°œì •ë²•ë¥ ì•ˆ.*|ì¼ë¶€ê°œì •.*|ì „ë¶€ê°œì •.*', '', title)
    cleaned_title = re.sub(r'\(.*?\)', '', cleaned_title).strip()
    query = urllib.parse.quote(cleaned_title)
    url = f"https://openapi.naver.com/v1/search/news?query={query}&display=100&start=1&sort=date"

    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", client_id)
    request.add_header("X-Naver-Client-Secret", client_secret)
    request.add_header("User-Agent", "Mozilla/5.0")

    try:
        response = urllib.request.urlopen(request)
        data = json.loads(response.read().decode("utf-8"))
        items = data.get("items", [])

        one_year_ago = datetime.now() - timedelta(days=800)
        article_candidates = []

        print(f"\nğŸ“Œ {title} (â†’ ê²€ìƒ‰ì–´: {cleaned_title})")

        title_emb = get_embedding(title)

        for item in items:
            raw_title = item["title"].replace("<b>", "").replace("</b>", "")
            link = item["link"]
            pubDate = item["pubDate"]

            try:
                pub_date = datetime.strptime(pubDate, "%a, %d %b %Y %H:%M:%S %z").replace(tzinfo=None)
                if pub_date < one_year_ago:
                    continue
            except:
                continue

            if "n.news.naver.com" not in link:
                continue

            sim = util.cos_sim(title_emb, get_embedding(raw_title)).item()
            if sim < sim_threshold:
                continue

            comment_count = get_comment_count(link)
            article_candidates.append((raw_title, link, comment_count, sim))
            printed_titles.append(raw_title)

        if article_candidates:
            best_article = sorted(article_candidates, key=lambda x: x[2], reverse=True)[0]
            max_comment = best_article[2]
            if max_comment >= 5:
                print(f"   âœ… {best_article[0]} ({max_comment}ê°œ, ìœ ì‚¬ë„: {best_article[3]:.2f}) â†’ {best_article[1]}")
                return True
            else:
                print(f"   âš ï¸ ë‰´ìŠ¤ {len(article_candidates)}ê°œ, ìµœëŒ€ ëŒ“ê¸€ìˆ˜ {max_comment}ê°œ, ìœ ì‚¬ë„ ìµœëŒ“ê°’ {best_article[3]:.2f} â†’ ì¡°ê±´ ë¯¸ì¶©ì¡±")
        else:
            print("   âŒ ì¡°ê±´ ì¶©ì¡± ë‰´ìŠ¤ ì—†ìŒ")

    except Exception as e:
        print(f"[ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜] {title}: {e}")

    return False




# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    age = 22
    init_db()

    # 1ï¸âƒ£ DB í™•ì¸
    titles = get_bills_by_age(age)
    titles = list(dict.fromkeys(titles)) #ì™„ì „í•œ ì¤‘ë³µ ì œê±° (ìµœì‹  ìˆœì„œ ìœ ì§€)


    if titles:
        print(f"[INFO] DBì—ì„œ {age}ëŒ€ êµ­íšŒ ë²•ì•ˆ {len(titles)}ê°œë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.\n")
    else:
        print(f"[INFO] DBì— {age}ëŒ€ êµ­íšŒ ë²•ì•ˆì´ ì—†ì–´, APIì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n")
        titles = get_bill_titles_by_age(age)

        print(f"[INFO] {age}ëŒ€ êµ­íšŒì—ì„œ {len(titles)}ê°œì˜ ë²•ì•ˆëª…ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n")

        # 2ï¸âƒ£ DBì— ì €ì¥ (ì¤‘ë³µ í™•ì¸ í¬í•¨)
        saved, skipped = 0, 0
        for title in titles:
            try:
                insert_bill(age, title)
                saved += 1
            except Exception as e:
                print(f"[DB ì €ì¥ ì‹¤íŒ¨] {title} â†’ {e}")
                skipped += 1

        print(f"[DB ì €ì¥ ê²°ê³¼] ì´ ì‹œë„: {len(titles)}, ì„±ê³µ: {saved}, ìŠ¤í‚µ: {skipped}\n")

    # 3ï¸âƒ£ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ì¶œë ¥
    for i, title in enumerate(titles, 1):
        result = search_news_unique(title)
        if result:
            print(f"ğŸ‘‰ {i}. {title} â†’ âœ… ëŒ“ê¸€ ë§ì€ ë‰´ìŠ¤ ìˆìŒ\n")
            
        time.sleep(0.1)

    driver.quit()
