import json
import urllib.request
import urllib.parse
import time
import re
from datetime import datetime, timedelta
from sentence_transformers import SentenceTransformer, util
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from insert_NewsScript import collect_and_store_missing_bodies
from dbmanage_News import (
    get_bills_by_year,
    insert_bill_by_year, 
    init_db, 
    insert_bill_news,
    get_news_by_bill_title,
    is_news_exist,
    is_exact_news_exist,
    insert_no_news_placeholder,
    update_missing_titles
    )


lock = Lock()
best_articles_by_title = {}



API_KEY = "68da180a494a4cc3b8add2071dc95242"
client_id = "CKb4pAJ84D6tVcCvpjka"
client_secret = "5PucVvnteo"
YEARS = list(range(2025, datetime.now().year + 1))  # 2016 ~ ì˜¬í•´
MAX_WORKERS = 8  # ë³‘ë ¬ ìŠ¤ë ˆë“œ ê°œìˆ˜ : 6~8 ì¶”ì²œ

# ì„ë² ë”© ëª¨ë¸ (ìŠ¤ë ˆë“œ ì•ˆì „)
model = SentenceTransformer("all-MiniLM-L6-v2")
embedding_cache = {}

def get_embedding(text: str):
    if text not in embedding_cache:
        embedding_cache[text] = model.encode(text, convert_to_tensor=True)
    return embedding_cache[text]

# Selenium ê¸°ë³¸ ì˜µì…˜ â€“ ìŠ¤ë ˆë“œë§ˆë‹¤ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì”€
base_options = Options()
base_options.add_argument("--headless=new")
base_options.add_argument("--no-sandbox")
base_options.add_argument("--disable-dev-shm-usage")

API_URL = "https://open.assembly.go.kr/portal/openapi/TVBPMBILL11"


def clean_url(url: str) -> str:
    url = url.strip()
    # ë¶ˆí•„ìš”í•œ query ì œê±° (ì˜ˆ: sid=100)
    parsed = urllib.parse.urlparse(url)
    cleaned = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
    return cleaned


def _get_bill_rows_by_age(age: int, p_size: int = 1000):
    """íŠ¹ì • *ëŒ€ìˆ˜*ì˜ ëª¨ë“  ë²•ì•ˆ rowë¥¼ ë°˜í™˜."""
    rows, p_index = [], 1
    while True:
        params = {
            "KEY": API_KEY,
            "Type": "json",
            "AGE": age,
            "pIndex": p_index,
            "pSize": p_size,
        }
        url = f"{API_URL}?{urllib.parse.urlencode(params)}"
        try:
            time.sleep(0.1)  # ë„ˆë¬´ ë¹ ë¥¸ í˜¸ì¶œ ë°©ì§€
            req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
            with urllib.request.urlopen(req, timeout=10) as resp:
                data = json.loads(resp.read())
            items = data.get("TVBPMBILL11", [])
            page_rows = items[1].get("row", []) if len(items) > 1 else []
            if not page_rows:
                break
            rows.extend(page_rows)
            if len(page_rows) < p_size:
                break
            p_index += 1
        except Exception as e:
            print(f"[API ì˜¤ë¥˜] {age}ëŒ€ {p_index}í˜ì´ì§€ â†’ {e}")
            break
    return rows


def get_bill_titles_by_year(year: int):
    """ì£¼ì–´ì§„ *year*(YYYY) ì— ë°œì˜ëœ ëª¨ë“  ë²•ì•ˆëª…ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜."""
    titles = []
    # 20â€§21â€§22ëŒ€ êµ­íšŒ ë²”ì£¼ì— ëª¨ë‘ ì§ˆì˜í•œ ë’¤ ë‚ ì§œë¡œ í•„í„°ë§
    for age in (20, 21, 22):
        for row in _get_bill_rows_by_age(age):
            propose_dt = row.get("PROPOSE_DT", "")  # YYYYMMDD í˜•íƒœ
            if propose_dt.startswith(str(year)):
                title = row.get("BILL_NAME", "").strip()
                if title:
                    titles.append(title)
    # ì¤‘ë³µ ì œê±° (ì›ë³¸ ìˆœì„œ ìœ ì§€)
    return list(dict.fromkeys(titles))



def get_comment_count(news_url: str, driver: webdriver.Chrome) -> int:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ URLì—ì„œ ëŒ“ê¸€ ìˆ˜ë¥¼ ë°˜í™˜."""
    try:
        driver.get(news_url)
        time.sleep(1)
        # "ë”ë³´ê¸°" ë²„íŠ¼ ìë™ í´ë¦­
        while True:
            try:
                more_btn = driver.find_element(By.CLASS_NAME, "u_cbox_btn_more")
                driver.execute_script("arguments[0].click();", more_btn)
                time.sleep(0.4)
            except Exception:
                break
        return len(driver.find_elements(By.CSS_SELECTOR, "span.u_cbox_contents"))
    except Exception as e:
        print(f"   [ëŒ“ê¸€ ì‹¤íŒ¨] {news_url} â†’ {e}")
        return 0


def search_news_unique(title: str, sim_threshold: float = 0.0): 
    # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ê¸°ëŠ¥ì´ ê´€ë ¨ì„±ì„ ì–´ëŠì •ë„ ë³´ì¥í•˜ë¯€ë¡œ 0.0 ìœ¼ë¡œ ìœ ì‚¬ë„ê¸°ì¤€ ì™„í™”
    # ë°˜ëŒ€ ì˜ë¯¸ì˜ ê¸°ì‚¬ì¼ ê²½ìš°ì—ë§Œ ê±¸ëŸ¬ë‚´ë„ë¡ í•¨ (ê±°ì˜ X)

    cleaned = re.sub(r"ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ.*|ì „ë¶€ê°œì •ë²•ë¥ ì•ˆ.*|ì¼ë¶€ê°œì •.*|ì „ë¶€ê°œì •.*", "", title)
    cleaned = re.sub(r"\(.*?\)", "", cleaned).strip()
    query = urllib.parse.quote(cleaned)
    url = f"https://openapi.naver.com/v1/search/news?query={query}&display=100&start=1&sort=date"
    req = urllib.request.Request(url)
    for k, v in (
        ("X-Naver-Client-Id", client_id),
        ("X-Naver-Client-Secret", client_secret),
        ("User-Agent", "Mozilla/5.0"),
    ):
        req.add_header(k, v)

    try:
        with urllib.request.urlopen(req) as resp:
            data = json.loads(resp.read().decode("utf-8"))
        items = data.get("items", [])
        cut_off = datetime.now() - timedelta(days=800)
        title_emb = get_embedding(title)

        best_article = None
        driver = webdriver.Chrome(options=base_options)

        for item in items:
            raw_title = item["title"].replace("<b>", "").replace("</b>", "")
            link = item["link"]
            pubDate = item["pubDate"]
            try:
                pub_dt = datetime.strptime(pubDate, "%a, %d %b %Y %H:%M:%S %z").replace(tzinfo=None)
                if pub_dt < cut_off:
                    continue
            except Exception:
                continue
            if "n.news.naver.com" not in link:
                continue

            sim = util.cos_sim(title_emb, get_embedding(raw_title)).item()
            if sim < sim_threshold:
                continue

            c_cnt = get_comment_count(link, driver)
            if c_cnt < 5:
                continue

            if (
                best_article is None or
                c_cnt > best_article[2] or
                (c_cnt == best_article[2] and sim > best_article[3])
            ):
                best_article = (raw_title, link, c_cnt, sim)

        driver.quit()
        return best_article
    except Exception as e:
        print(f"[ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜] {title}: {e}")
        return None



# ìŠ¤ë ˆë“œ ì‘ì—… í•¨ìˆ˜
def process_title(index: int, title: str, year: int):
    """ë‰´ìŠ¤ DBì— ì €ì¥ë˜ì§€ ì•Šì€ ë²•ì•ˆì— ëŒ€í•´ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ì €ì¥"""
    result = search_news_unique(title)
    if result:
        best_title, best_link, c_cnt, sim = result
        best_articles_by_title[title] = result

        # ğŸ”½ ë¨¼ì € URL ì •ì œ
        cleaned_url = clean_url(best_link)

        # âœ… ì¤‘ë³µ ì—¬ë¶€ ë¨¼ì € ì²´í¬
        if is_exact_news_exist(title, year, best_title, cleaned_url):
            print(f"\n[{index+1:03}] â© ì´ë¯¸ ì €ì¥ëœ ë‰´ìŠ¤ â†’ ìŠ¤í‚µ")
            return

        # âœ… íƒìƒ‰ ê²°ê³¼ ì¶œë ¥
        print(f"\n[{index+1:03}] âœ… {best_title} ({c_cnt}ê°œ, ìœ ì‚¬ë„ {sim:.3f})")
        print(f"[{index+1:03}] {title} â†’ ğŸ” ìµœë‹¤ ëŒ“ê¸€ ë‰´ìŠ¤ê¸°ì‚¬ ë§í¬ : {best_link}")

        # âœ… ë‰´ìŠ¤ ì €ì¥
        insert_bill_news(
            bill_title=title,
            year=year,
            news_title=best_title,
            news_url=cleaned_url,  # ì •ì œëœ URLë¡œ ì €ì¥
            comment_count=c_cnt,
            similarity=sim,
        )


    else:
        print(f"\n[{index+1:03}] âŒ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ: {title}")
        insert_no_news_placeholder(title, year)  # â† ì´ ì¤„ ì¶”ê°€





if __name__ == "__main__":
    init_db()

    update_missing_titles()
    # bill_news ì— bills ì˜ title ì¹¼ëŸ¼ì—ì„œ ë²•ì•ˆëª… ê°€ì ¸ì˜´ (db ë°ì´í„° ë¬´ê²°ì„± í™•ì¸ìš©)

    for year in YEARS:
        print(f"\n==================== {year}ë…„ ë²•ì•ˆ ====================")
        titles = get_bills_by_year(year)
        if titles:
            print(f"[INFO] DBì—ì„œ {year}ë…„ ë²•ì•ˆ {len(titles)}ê°œë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤. \n")
        else:
            print(f"[INFO] DBì— {year}ë…„ ìë£Œê°€ ì—†ì–´ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. \n")
            titles = get_bill_titles_by_year(year)
            print(f"[INFO] {year}ë…„ ë²•ì•ˆ {len(titles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ. \n")
            saved = skipped = 0
            for t in titles:
                try:
                    insert_bill_by_year(year, t)
                    saved += 1
                except Exception as e:
                    print(f"[DB ì €ì¥ ì‹¤íŒ¨] {t} â†’ {e}")
                    skipped += 1
            print(f"[DB ì €ì¥ ê²°ê³¼] ì‹œë„: {len(titles)}, ì„±ê³µ: {saved}, ìŠ¤í‚µ: {skipped}")

        title_to_index = {}

        # ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë²•ì•ˆë§Œ í•„í„°ë§
        titles_without_news = []
        for idx, title in enumerate(titles):
            title_to_index[title] = idx
            exists = is_news_exist(title, year)
            print(f"ë‰´ìŠ¤ ì¡´ì¬ ì—¬ë¶€: {title} ({year}) â†’ {exists}")
            if not exists:
                titles_without_news.append(title)

        # DBì—ì„œ ì´ë¯¸ ë‰´ìŠ¤ê°€ ì €ì¥ëœ ë²•ì•ˆì— ëŒ€í•œ ì¶œë ¥
        printed_idx = 1  # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì¶œë ¥ìš© ì¸ë±ìŠ¤

        for title in set(titles) - set(titles_without_news):
            news_list = get_news_by_bill_title(title, year)
            for news_title, news_url in news_list:
                print(f"\n[{printed_idx:03}] ğŸ—‚ï¸  {news_title} (ì´ë¯¸ ì €ì¥ë¨)")
                print(f"[{printed_idx:03}] {title} â†’ ğŸ” ë‰´ìŠ¤ê¸°ì‚¬ ë§í¬ : {news_url}")
                printed_idx += 1

        # ğŸ”„ ì €ì¥ë˜ì§€ ì•Šì€ ë‰´ìŠ¤ëŠ” ë³‘ë ¬ ì²˜ë¦¬
        if titles_without_news:
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = [
                    executor.submit(process_title, title_to_index[t], t, year)
                    for t in titles_without_news
                ]
                for future in as_completed(futures):
                    try:
                        future.result()
                    except Exception as e:
                        print(f"[ìŠ¤ë ˆë“œ ì˜ˆì™¸ ë°œìƒ] {e}")
                        import traceback
                        traceback.print_exc()

        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        for title in titles:
            article = best_articles_by_title.get(title)
            if article:
                best_title, url, c_cnt, sim = article
                if c_cnt >= 5:
                    print(f"\n   âœ… {best_title} ({c_cnt}ê°œ, ìœ ì‚¬ë„ {sim:.3f})")
                    print(f"{title} â†’ ğŸ” ìµœë‹¤ ëŒ“ê¸€ ë‰´ìŠ¤ê¸°ì‚¬ ë§í¬ : {url}")


        time.sleep(2)


    collect_and_store_missing_bodies() # ì¼ë‹¨ 1000ê°œë¡œ ì˜ˆìƒ
    print("\në³¸ë¬¸ db ì €ì¥ ì™„ë£Œ")



        

